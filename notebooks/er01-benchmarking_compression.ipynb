{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1048a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import multiprocessing as mp\n",
    "import shutil\n",
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import LocalCluster\n",
    "from dask.distributed import performance_report\n",
    "\n",
    "from confscale.parser import read_smi_to_dask\n",
    "from confscale.parser import smi2parquet\n",
    "from confscale.parser import write_dask_to_parquet\n",
    "from confscale.utils import compute_dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048ed516",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/raw/ZINC20_drug_like\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d94495",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"smiles\", pa.string()),\n",
    "        pa.field(\"zinc_id\", pa.uint32()),\n",
    "        pa.field(\"tranches\", pa.string()),\n",
    "    ],\n",
    "    metadata={\n",
    "        \"description\": \"ZINC20 subset\",\n",
    "        \"Predefined_Subsets\": \"Drug-Like\",\n",
    "        \"Highest_Reactivity\": \"Anodyne\",\n",
    "        \"Highest_Activity_exclusive\": \"True\",\n",
    "        \"Minimum_Purchasability\": \"Wait_Ok\",\n",
    "        \"Minimum_Purchasability_exclusive\": \"True\",\n",
    "        \"source\": \"https://zinc20.docking.org/\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0ea2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = smi2parquet(dataset_path=dataset_path, schema=schema, partitioning=ds.partitioning(pa.schema([(\"tranches\", pa.string())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e162ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_path = \"../data/processed/ZINC20_drug_like\"\n",
    "minrowpergroup = 10**6\n",
    "maxrowpergroup = 2 * 10**6\n",
    "maxrowperfile = 100 * 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66415dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.set_io_thread_count(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be8e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset_to_parquet(\n",
    "    dataset: ds.Dataset,\n",
    "    output_path: str,\n",
    "    min_rows_per_group: int = 10**5,\n",
    "    max_rows_per_group: int = 1024 * 1024,\n",
    "    max_rows_per_file: int = 100 * 10**6,\n",
    "    compression: str = \"zstd\",\n",
    "    compression_level: int = 3,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes a PyArrow dataset to Parquet format with specified parameters.\n",
    "\n",
    "    Args:\n",
    "        dataset (ds.Dataset): The PyArrow dataset to write.\n",
    "        output_path (str): Path where the Parquet dataset will be written.\n",
    "        min_rows_per_group (int): Minimum number of rows per row group.\n",
    "        max_rows_per_group (int): Maximum number of rows per row group.\n",
    "        max_rows_per_file (int): Maximum number of rows per file.\n",
    "        compression (str): Compression algorithm to use.\n",
    "        compression_level (int): Compression level.\n",
    "    \"\"\"\n",
    "    ds.write_dataset(\n",
    "        dataset,\n",
    "        output_path,\n",
    "        format=\"parquet\",\n",
    "        min_rows_per_group=min_rows_per_group,\n",
    "        max_rows_per_group=max_rows_per_group,\n",
    "        max_rows_per_file=max_rows_per_file,\n",
    "        file_options=ds.ParquetFileFormat().make_write_options(compression=compression, compression_level=compression_level),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ce18eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d252e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_compression(\n",
    "    dataset: ds.Dataset,\n",
    "    output_path: str,\n",
    "    compression: str,\n",
    "    compression_level: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmarks different compression algorithms for Parquet writing.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input SMI files.\n",
    "        output_path (str): Path to the output Parquet files.\n",
    "        compression (str): Compression algorithm to use (e.g., \"snappy\", \"gzip\", \"brotli\", \"zstd\", \"lz4\").\n",
    "        compression_level (int, optional): Compression level for algorithms that support it. Defaults to None.\n",
    "        blocksize (str): Block size for reading the SMI files.\n",
    "        sep (str): Separator used in the SMI files.\n",
    "    \"\"\"\n",
    "\n",
    "    result_dict = {\n",
    "        \"compression\": compression,\n",
    "        \"compression_level\": compression_level,\n",
    "        \"duration\": None,\n",
    "        \"input_size\": compute_dataset_size(dataset),\n",
    "        \"output_size\": None,\n",
    "        \"compression_ratio\": None,\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Write the dataset to Parquet format with the specified compression\n",
    "    write_dataset_to_parquet(\n",
    "        dataset,\n",
    "        output_path,\n",
    "        min_rows_per_group=10**5,\n",
    "        max_rows_per_group=1024 * 1024,\n",
    "        max_rows_per_file=100 * 10**6,\n",
    "        compression=compression,\n",
    "        compression_level=compression_level,\n",
    "    )\n",
    "\n",
    "    result_dict[\"duration\"] = time.time() - start_time\n",
    "\n",
    "    # get size info\n",
    "\n",
    "    result_dict[\"output_size\"] = compute_dataset_size(ds.dataset(output_path, format=\"parquet\"))\n",
    "    result_dict[\"compression_ratio\"] = result_dict[\"input_size\"] / result_dict[\"output_size\"]\n",
    "\n",
    "    # clean up output directory\n",
    "    if Path(output_path).exists():\n",
    "        shutil.rmtree(output_path)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "593687d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_infos = [\n",
    "    # {\n",
    "    #     \"compression\" : \"snappy\",\n",
    "    #     \"compression_level\" : None,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"compression\": \"gzip\",\n",
    "    #     \"compression_level\": range(1, 10),\n",
    "    # },\n",
    "    # {\n",
    "    #     \"compression\": \"brotli\",\n",
    "    #     \"compression_level\": range(12),\n",
    "    # },\n",
    "    {\n",
    "        \"compression\": \"zstd\",\n",
    "        \"compression_level\": range(1, 23),\n",
    "    },\n",
    "    {\n",
    "        \"compression\": \"lz4\",\n",
    "        \"compression_level\": range(1, 13),\n",
    "    },\n",
    "]\n",
    "# Initialize list to store benchmark results\n",
    "bench_mark_results = []\n",
    "\n",
    "# For each compression algorithm and level combination\n",
    "for info in compression_infos:\n",
    "    compression_algorithm = info[\"compression\"]\n",
    "    compression_levels = info[\"compression_level\"]\n",
    "    algorithm_results = []\n",
    "\n",
    "    # Create directory for this compression algorithm\n",
    "    output_dir = Path(\"../reports/ZINC20_compression\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Test with different compression levels if specified\n",
    "    if compression_levels is None:\n",
    "        result = benchmark_compression(\n",
    "            dataset=data,\n",
    "            output_path=f\"../data/processed/ZINC20_{compression_algorithm}_test\",\n",
    "            compression=compression_algorithm,\n",
    "            compression_level=None,\n",
    "        )\n",
    "        algorithm_results.append(result)\n",
    "        bench_mark_results.append(result)\n",
    "    else:\n",
    "        # For each compression level in the range\n",
    "        for level in compression_levels:\n",
    "            result = benchmark_compression(\n",
    "                dataset=data,\n",
    "                output_path=f\"../data/processed/ZINC20_{compression_algorithm}_{level}_test\",\n",
    "                compression=compression_algorithm,\n",
    "                compression_level=level,\n",
    "            )\n",
    "            algorithm_results.append(result)\n",
    "            bench_mark_results.append(result)\n",
    "\n",
    "    # Save results for this compression algorithm immediately\n",
    "    algorithm_df = pd.DataFrame(algorithm_results)\n",
    "    algorithm_df.to_csv(output_dir / f\"benchmark_results_parquet_pyarrow_{compression_algorithm}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58eda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_infos = [\n",
    "    # {\n",
    "    #     \"compression\" : \"snappy\",\n",
    "    #     \"compression_level\" : None,\n",
    "    # },\n",
    "    # {\n",
    "    #     \"compression\": \"gzip\",\n",
    "    #     \"compression_level\": range(1, 10),\n",
    "    # },\n",
    "    {\n",
    "        \"compression\": \"brotli\",\n",
    "        \"compression_level\": range(12),\n",
    "    },\n",
    "    # {\n",
    "    #     \"compression\": \"zstd\",\n",
    "    #     \"compression_level\": range(1, 23),\n",
    "    # },\n",
    "    # {\n",
    "    #     \"compression\": \"lz4\",\n",
    "    #     \"compression_level\": range(1, 13),\n",
    "    # },\n",
    "]\n",
    "# Initialize list to store benchmark results\n",
    "bench_mark_results = []\n",
    "\n",
    "# For each compression algorithm and level combination\n",
    "for info in compression_infos:\n",
    "    compression_algorithm = info[\"compression\"]\n",
    "    compression_levels = info[\"compression_level\"]\n",
    "    algorithm_results = []\n",
    "\n",
    "    # Create directory for this compression algorithm\n",
    "    output_dir = Path(\"../reports/ZINC20_compression\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Test with different compression levels if specified\n",
    "    if compression_levels is None:\n",
    "        result = benchmark_compression(\n",
    "            dataset=data,\n",
    "            output_path=f\"../data/processed/ZINC20_{compression_algorithm}_test\",\n",
    "            compression=compression_algorithm,\n",
    "            compression_level=None,\n",
    "        )\n",
    "        algorithm_results.append(result)\n",
    "        bench_mark_results.append(result)\n",
    "    else:\n",
    "        # For each compression level in the range\n",
    "        for level in compression_levels:\n",
    "            result = benchmark_compression(\n",
    "                dataset=data,\n",
    "                output_path=f\"../data/processed/ZINC20_{compression_algorithm}_{level}_test\",\n",
    "                compression=compression_algorithm,\n",
    "                compression_level=level,\n",
    "            )\n",
    "            algorithm_results.append(result)\n",
    "            bench_mark_results.append(result)\n",
    "\n",
    "    # Save results for this compression algorithm immediately\n",
    "    algorithm_df = pd.DataFrame(algorithm_results)\n",
    "    algorithm_df.to_csv(output_dir / f\"benchmark_results_parquet_pyarrow_{compression_algorithm}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5210bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=18, memory_limit=\"1.5GB\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfdd53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576b2238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b83a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_compression(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    compression_algorithm: str,\n",
    "    compression_level: int | None = None,\n",
    "    blocksize: str = \"128MB\",\n",
    "    sep: str = \" \",\n",
    "    schema: pa.Schema | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmarks different compression algorithms for Parquet writing.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input SMI files.\n",
    "        output_path (str): Path to the output Parquet files.\n",
    "        compression (str): Compression algorithm to use (e.g., \"snappy\", \"gzip\", \"brotli\", \"zstd\", \"lz4\").\n",
    "        compression_level (int, optional): Compression level for algorithms that support it. Defaults to None.\n",
    "        blocksize (str): Block size for reading the SMI files.\n",
    "        sep (str): Separator used in the SMI files.\n",
    "    \"\"\"\n",
    "\n",
    "    result_dict = {\n",
    "        \"compression_algorithm\": compression_algorithm,\n",
    "        \"compression_level\": compression_level,\n",
    "        \"blocksize\": blocksize,\n",
    "        \"duration\": None,\n",
    "        \"input_size\": None,\n",
    "        \"output_size\": None,\n",
    "        \"compression_ratio\": None,\n",
    "        \"report_path\": None,\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_path = Path(input_path)\n",
    "\n",
    "    experience_name = \"_\".join([input_path.stem, compression_algorithm, str(compression_level), str(blocksize), \"dask-report.html\"])\n",
    "\n",
    "    report_dir = Path(f\"../reports/ZINC20_compression/{compression_algorithm}\")\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    report_path = report_dir / experience_name\n",
    "    result_dict[\"report_path\"] = report_path\n",
    "\n",
    "    start_time = time.time()\n",
    "    with performance_report(filename=str(report_path)):\n",
    "        ddf = read_smi_to_dask(\n",
    "            str(Path(input_path) / \"*/*.smi\"),\n",
    "            blocksize=blocksize,\n",
    "            sep=sep,\n",
    "        )\n",
    "        parquet_writer = write_dask_to_parquet(\n",
    "            ddf,\n",
    "            output_path,\n",
    "            compression=compression_algorithm,\n",
    "            level_compression=compression_level,\n",
    "            schema=schema,\n",
    "        )\n",
    "        parquet_writer.compute()\n",
    "    result_dict[\"duration\"] = time.time() - start_time\n",
    "\n",
    "    # get size info\n",
    "    input_size = compute_dataset_size(\n",
    "        smi2parquet(\n",
    "            dataset_path=input_path,\n",
    "            schema=schema,\n",
    "            partitioning=ds.partitioning(pa.schema([(\"tranches\", pa.string())])),\n",
    "        )\n",
    "    )\n",
    "    output_size = compute_dataset_size(ds.dataset(output_path, format=\"parquet\"))\n",
    "    result_dict[\"input_size\"] = input_size\n",
    "    result_dict[\"output_size\"] = output_size\n",
    "    result_dict[\"compression_ratio\"] = input_size / output_size\n",
    "\n",
    "    # clean up output directory\n",
    "    if Path(output_path).exists():\n",
    "        shutil.rmtree(output_path)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90821496",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'benchmark_compression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m compression_level:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m blocksize:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         result = \u001b[43mbenchmark_compression\u001b[49m(\n\u001b[32m     40\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m../data/raw/ZINC20_drug_like/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m../data/processed/zinc20_drug_like/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m             compression_algorithm=compression_algorithm,\n\u001b[32m     43\u001b[39m             compression_level=level,\n\u001b[32m     44\u001b[39m             blocksize=block,\n\u001b[32m     45\u001b[39m         )\n\u001b[32m     46\u001b[39m         bench_mark_results.append(result)\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Convert the benchmark results to a DataFrame\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'benchmark_compression' is not defined"
     ]
    }
   ],
   "source": [
    "compression_infos = [\n",
    "    # {\n",
    "    #     \"compression\" : \"snappy\",\n",
    "    #     \"compression_level\" : None,\n",
    "    #     \"blocksize\" : [\"16MB\", \"32MB\", \"64MB\"],\n",
    "    # },\n",
    "    # {\n",
    "    #     \"compression\": \"gzip\",\n",
    "    #     \"compression_level\": range(1, 10),\n",
    "    #     \"blocksize\": [\"16MB\", \"32MB\", \"64MB\"],\n",
    "    # },\n",
    "    # {\n",
    "    #     \"compression\": \"brotli\",\n",
    "    #     \"compression_level\": range(12),\n",
    "    #     \"blocksize\": [\"16MB\", \"32MB\", \"64MB\"],\n",
    "    # },\n",
    "    {\n",
    "        \"compression\": \"zstd\",\n",
    "        \"compression_level\": range(1, 23),\n",
    "        \"blocksize\": [\"16MB\", \"32MB\", \"64MB\"],\n",
    "    },\n",
    "    {\n",
    "        \"compression\": \"lz4\",\n",
    "        \"compression_level\": range(1, 13),\n",
    "        \"blocksize\": [\"16MB\", \"32MB\", \"64MB\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "for info in compression_infos:\n",
    "    bench_mark_results = []\n",
    "    compression_algorithm = info[\"compression\"]\n",
    "    compression_level = info[\"compression_level\"]\n",
    "    blocksize = info[\"blocksize\"]\n",
    "\n",
    "    if isinstance(compression_level, range):\n",
    "        for level in compression_level:\n",
    "            for block in blocksize:\n",
    "                result = benchmark_compression(\n",
    "                    \"../data/raw/ZINC20_drug_like/\",\n",
    "                    \"../data/processed/zinc20_drug_like/\",\n",
    "                    compression_algorithm=compression_algorithm,\n",
    "                    compression_level=level,\n",
    "                    blocksize=block,\n",
    "                )\n",
    "                bench_mark_results.append(result)\n",
    "            # Convert the benchmark results to a DataFrame\n",
    "        benchmark_df = pd.DataFrame(bench_mark_results)\n",
    "        # Save the benchmark results to a CSV file\n",
    "        benchmark_df.to_csv(\n",
    "            f\"../reports/ZINC20_compression/benchmark_results_dask_{compression_algorithm}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "    else:\n",
    "        for block in blocksize:\n",
    "            result = benchmark_compression(\n",
    "                \"../data/raw/ZINC20_drug_like/\",\n",
    "                \"../data/processed/zinc20_drug_like/\",\n",
    "                compression_algorithm=compression_algorithm,\n",
    "                compression_level=compression_level,\n",
    "                blocksize=block,\n",
    "            )\n",
    "            bench_mark_results.append(result)\n",
    "            # Convert the benchmark results to a DataFrame\n",
    "        benchmark_df = pd.DataFrame(bench_mark_results)\n",
    "        # Save the benchmark results to a CSV file\n",
    "        benchmark_df.to_csv(\n",
    "            f\"../reports/ZINC20_compression/benchmark_results_dask_{compression_algorithm}.csv\",\n",
    "            index=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip_compress(file_path: Path | str, output_folder: Path | str, compresslevel: int = 9) -> None:\n",
    "    \"\"\"\n",
    "    Compresses a text file using gzip compression.\n",
    "\n",
    "    Args:\n",
    "        file_path (str|Path): Path to the input text file.\n",
    "        output_folder (str): Path to the output gzip file.\n",
    "        compresslevel (int): Compression level for gzip (1-9, default is 9).\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path) if isinstance(file_path, str) else file_path\n",
    "    output_folder = Path(output_folder) if isinstance(output_folder, str) else output_folder\n",
    "\n",
    "    if not isinstance(file_path, Path):\n",
    "        raise ValueError(\"input_file must be a Path or string object\")\n",
    "    if not isinstance(output_folder, Path):\n",
    "        raise ValueError(\"output_folder must be a Path or string object\")\n",
    "\n",
    "    output_file_path = Path(output_folder) / (file_path.stem + \".gz\")\n",
    "    with file_path.open(\"rb\") as f_in:\n",
    "        with gzip.open(output_file_path, \"wb\", compresslevel=compresslevel) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "def benchmark_gzip_compression(input_folder: str, output_folder: str, compresslevel: int = 9):\n",
    "    \"\"\"\n",
    "    Benchmarks gzip compression for a given text file.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input text file.\n",
    "        output_file (str): Path to the output gzip file.\n",
    "        compresslevel (int): Compression level for gzip (1-9, default is 9).\n",
    "    \"\"\"\n",
    "    result_dict = {\n",
    "        \"compression\": \"gzip\",\n",
    "        \"compression_level\": compresslevel,\n",
    "        \"duration\": None,\n",
    "        \"input_size\": None,\n",
    "        \"output_size\": None,\n",
    "        \"compression_ratio\": None,\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # aggregate all  smi files in the input folder\n",
    "    input_file_path = Path(input_folder).glob(\"**/*.smi\")\n",
    "\n",
    "    # create the output folder if it doesn't exist\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Compress each file in the input folder\n",
    "    for file_path in input_file_path:\n",
    "        gzip_compress(file_path, output_folder, compresslevel=compresslevel)\n",
    "\n",
    "    result_dict[\"duration\"] = time.time() - start_time\n",
    "    # get size info\n",
    "    result_dict[\"input_size\"] = sum([file.stat().st_size for file in Path(input_folder).glob(\"**/*.smi\")])\n",
    "    result_dict[\"output_size\"] = sum([file.stat().st_size for file in output_folder.glob(\"*.gz\")])\n",
    "    result_dict[\"compression_ratio\"] = result_dict[\"input_size\"] / result_dict[\"output_size\"]\n",
    "\n",
    "    # Clean up the output directory\n",
    "    if output_folder.exists():\n",
    "        shutil.rmtree(output_folder)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def benchmark_parralel_gzip_compression(input_folder: str, output_folder: str, compresslevel: int = 9, nb_workers: int = 4):\n",
    "    \"\"\"\n",
    "    Benchmarks gzip compression for a database of smi files in paralel.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input text file.\n",
    "        output_file (str): Path to the output gzip file.\n",
    "        compresslevel (int): Compression level for gzip (1-9, default is 9).\n",
    "        nb_workers (int): Number of workers to use for parallel processing.\n",
    "    \"\"\"\n",
    "    result_dict = {\n",
    "        \"compression\": \"gzip\",\n",
    "        \"compression_level\": compresslevel,\n",
    "        \"duration\": None,\n",
    "        \"input_size\": None,\n",
    "        \"output_size\": None,\n",
    "        \"compression_ratio\": None,\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # aggregate all  smi files in the input folder\n",
    "    input_file_path = Path(input_folder)\n",
    "    input_file_path = input_file_path.glob(\"**/*.smi\")\n",
    "\n",
    "    # create the output folder if it doesn't exist\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create partial function for parallel processing\n",
    "    partial_gzip_compress = partial(gzip_compress, output_folder=output_folder, compresslevel=compresslevel)\n",
    "\n",
    "    # Use multiprocessing to compress files in parallel\n",
    "    with mp.Pool(processes=nb_workers) as pool:\n",
    "        pool.map(partial_gzip_compress, input_file_path)\n",
    "\n",
    "    result_dict[\"duration\"] = time.time() - start_time\n",
    "    result_dict[\"input_size\"] = sum([file.stat().st_size for file in Path(input_folder).glob(\"**/*.smi\")])\n",
    "    result_dict[\"output_size\"] = sum([file.stat().st_size for file in output_folder.glob(\"*.gz\")])\n",
    "    result_dict[\"compression_ratio\"] = result_dict[\"input_size\"] / result_dict[\"output_size\"]\n",
    "\n",
    "    # Clean up the output directory\n",
    "    if output_folder.exists():\n",
    "        shutil.rmtree(output_folder)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c54f0248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/raw/ZINC20_drug_like/FB/FBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GI/GIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CG/CGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BD/BDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FF/FFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HB/HBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HH/HHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HD/HDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EJ/EJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CH/CHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GC/GCAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BI/BIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EH/EHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/II/IIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EB/EBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JE/JEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IC/ICAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CA/CAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JF/JFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EA/EAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DB/DBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EC/ECAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JD/JDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CI/CIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JA/JAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CC/CCAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JC/JCAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CD/CDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GA/GAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FJ/FJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EI/EIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FI/FIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CE/CEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FD/FDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DJ/DJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IF/IFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IG/IGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BH/BHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GJ/GJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FC/FCAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HF/HFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IJ/IJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GB/GBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FE/FEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HJ/HJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HA/HAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DC/DCAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JB/JBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FH/FHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GF/GFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CB/CBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BJ/BJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JI/JIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JJ/JJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IH/IHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FG/FGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CF/CFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DF/DFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BE/BEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BF/BFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EF/EFAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IB/IBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DE/DEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BA/BAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BC/BCAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JH/JHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HG/HGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/JG/JGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BB/BBAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HE/HEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HI/HIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/ED/EDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DD/DDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IE/IEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EE/EEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/ID/IDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GE/GEAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DI/DIAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DA/DAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DG/DGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/HC/HCAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GH/GHAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/FA/FAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/BG/BGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/CJ/CJAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/IA/IAAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GD/GDAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/EG/EGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/GG/GGAD.smi'),\n",
       " PosixPath('../data/raw/ZINC20_drug_like/DH/DHAD.smi')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Path(\"../data/raw/ZINC20_drug_like/\").glob(\"**/*.smi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96417696",
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip_results = []\n",
    "compresslevel = range(1, 10)\n",
    "for level in compresslevel:\n",
    "    result = benchmark_gzip_compression(\n",
    "        \"../data/raw/ZINC20_drug_like/\",\n",
    "        \"../data/processed/zinc20_drug_like/\",\n",
    "        compresslevel=level,\n",
    "    )\n",
    "    gzip_results.append(result)\n",
    "\n",
    "# Convert the benchmark results to a DataFrame\n",
    "benchmark_df = pd.DataFrame(gzip_results)\n",
    "# Save the benchmark results to a CSV file\n",
    "benchmark_df.to_csv(\n",
    "    \"../reports/ZINC20_compression/benchmark_results_text_gzip.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce30207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark parralel gzip compression with various compression levels and worker counts\n",
    "parralel_gzip_results = []\n",
    "\n",
    "# Try different compression levels\n",
    "compresslevel_range = range(1, 10)\n",
    "worker_counts = [4, 8, 16]  # Different number of workers to test\n",
    "\n",
    "for level in compresslevel_range:\n",
    "    for workers in worker_counts:\n",
    "        # Add a descriptive print to track progress\n",
    "        # print(f\"Benchmarking parallel gzip with level {level} using {workers} workers\")\n",
    "\n",
    "        result = benchmark_parralel_gzip_compression(\n",
    "            \"../data/raw/ZINC20_drug_like/\", \"../data/processed/zinc20_drug_like_parallel/\", compresslevel=level, nb_workers=workers\n",
    "        )\n",
    "        # Add number of workers to the result dictionary\n",
    "        result[\"nb_workers\"] = workers\n",
    "        parralel_gzip_results.append(result)\n",
    "\n",
    "# Convert the benchmark results to a DataFrame\n",
    "parralel_benchmark_df = pd.DataFrame(parralel_gzip_results)\n",
    "\n",
    "# Save the benchmark results to a CSV file\n",
    "parralel_benchmark_df.to_csv(\n",
    "    \"../reports/ZINC20_compression/benchmark_results_text_parralel_gzip.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confscale-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
